{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numerical Analysis (Math 445 and 545)\n",
    "\n",
    "## Lecture 7 - Errors and Condition Numbers\n",
    "\n",
    "**Author: Daniel Poll**\n",
    "<br>\n",
    "**Date: February 3, 2020**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivation\n",
    "\n",
    "In the previous lecture, we saw that solutions to the system of linear equations $Ax = b$ can be found rather quickly through a Gaussian elimination-type algorithm called LU decomposition. However, we made many assumptions in that lecture, such as $A$ being a square matrix and $A$ being nonsingular. On top of that, we saw that numerical implementation can lead to round-off error. It makes sense that we have a way to check if this may be an issue. \n",
    "\n",
    "\n",
    "Further, if we break the assumption that $A$ is $n \\times n$, then we are not guaranteed a solution. In fact, we may have multiple solutions, a unique solution, or no solution at all. Most of our focus in the next few lectures will be on *overdetermined* systems. Indeed, these are systems that most students have already seen. For example, linear regression requires fitting $n$ points to an equation which only has two variables: a slope and a y-intercept. This can be phrased an overdetermined system (discussed in more detail later).\n",
    "\n",
    "So, it's in our best interest to discuss techniques that can be used when we have such a system. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix Errors and Norms\n",
    "\n",
    "For an approximation $x_a$ of the solution $x$ of $A x = b$, the *residual* $r = A x_a - b$ measures error as *backward error*, often measured by a single number, the residual norm $\\| A x_a - b \\|$.\n",
    "Any norm could be used, but the maximum norm or the 2-norm is usually preferred, for reasons that we will see soon.\n",
    "\n",
    "The corresponding (dimensionless) measure of relative error is defined as\n",
    "$$\\frac{\\|r\\|}{\\|b\\|}.$$\n",
    "\n",
    "However, these can greatly underestimate the *forward* errors in the solution: the absolute error $\\|x - x_a\\|$ and relative error\n",
    "$$Rel(x_a) = \\frac{\\|x - x_a\\|}{\\| x \\|}$$\n",
    "\n",
    "To relate these to the residual, we need the concepts of a *matrix norm* and the *condition number* of a matrix.\n",
    "\n",
    "These induced matrix norms have many properties in common with Euclidean length and other vector norms, but there can also be products, and then one has to be careful.\n",
    "\n",
    "1. $\\|A\\| \\geq 0$ (positivity)\n",
    "1. $\\| A \\| = 0$ if and only if $A = 0$ (definiteness)\n",
    "2. $\\| c A \\| = |c| \\, \\|A\\|$ for any constant $c$ (absolute homogeneity)\n",
    "3. $\\| A + B \\| \\leq \\| A \\| + \\| B \\|$ (sub-additivity or the triangle inequality),\n",
    "<br>\n",
    "and when the product of two matrices makes sense (including matrix-vector products),\n",
    "4. $\\| A B \\| \\leq \\| A \\| \\, \\| B \\|$ (sub-multiplicativity)\n",
    "\n",
    "Note the failure to always have equality with products.\n",
    "Indeed one can have $A B = 0$ with $A$ and $B$ both non-zero, such as when $A$ is a singular matrix and $B$ is a null-vector for it.\n",
    "\n",
    "Two common choices for matrix norms:\n",
    "\n",
    "1. Given any vector norm $\\| \\cdot \\|$ — such as the maximum (\"infinity\") norm $\\| \\cdot \\|_\\infty$ or the Euclidean norm (length) $\\| \\cdot \\|_2$ — the correponding *induced matrix norm* is\n",
    "$$\n",
    "\\| A \\| := \\max_{x \\neq 0} \\frac{\\| Ax \\|}{\\| x \\|}, =  \\max_{\\|x\\|=1} \\| Ax \\|\n",
    "$$\n",
    "This maximum exists for the infinity norm, and indeed there ia an explicit formula for it:\n",
    "for any $m\\times n$ matrix,\n",
    "$$\n",
    "\\|A\\|_\\infty = \\max_{i=1}^m \\sum_{j=1}^n |a_{ij}|\n",
    "$$\n",
    "(On the other hand, it is far harder to compute the Euclidean norm of a matrix: the formula requires computing eigenvalues.)\n",
    "\n",
    "Note that when the matrix is a vector considered as amatrix with a single column — so $n=1$ — the sum goes away, and this agrees with the infinity vector norm.\n",
    "This allows us to consider vectors as being just matrices with a single column, which we will often do from now on.\n",
    "\n",
    "2. Due to the difficulty of the induced 2-norm, a popular alternative is the *Frobenius norm*. We usually denote the Frobenius norm with a subscript $F$ and write\n",
    "$$\n",
    "||A||_F = \\sqrt{ \\sum_{i=1}^m \\sum_{j=1}^n |a_{ij}|^2} = \\sqrt{ \\text{trace}\\big(A^*A\\big) }\n",
    "$$\n",
    "Note that this is *not* the norm that is induced by the vector 2-norm, although that may look to be the case at first glance. In fact, it can be shown that the Frobenius norm is an upper bound for the induced 2-norm. So, it is a good alternative. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Condition Numbers\n",
    "\n",
    "It can be proven that, that if $A$ is square and invertible, then for any choice of norm,\n",
    "\n",
    "$$\n",
    "Rel(x_a) = \\frac{\\|x - x_a\\|}{\\| x \\|} \\leq \\|A\\|\\|A^{-1}\\|\\frac{\\|r\\|}{\\|b\\|},\n",
    "$$\n",
    "\n",
    "where the last factor is the relative backward error. We can do this by recognizing two main points. If \n",
    "\n",
    "1. If $A(x-x_a) = r$, then $\\|x-x_a\\| \\leq \\|A^{-1}\\|\\|r\\|$\n",
    "2. If $Ax = b$, then $\\|A\\|\\|x\\| \\geq \\|b\\|$. \n",
    "\n",
    "Combining these two inequalities will give the result.\n",
    "\n",
    "Since we can (though often with considerable effort, due to the inverse!) compute the right-hand side when the infinity norm is used, we can compute an upper bound on the relative error.\n",
    "Form this, an upper bound on the absolute error can be computed if needed.\n",
    "\n",
    "The *growth factor* between the relative backward error measured by the residual and the relative (forward) error is called the *condition number*, $K(A)$:\n",
    "$$\n",
    "\\kappa(A) := \\|A\\| \\|A^{-1}\\|\n",
    "$$\n",
    "Note that if $A$ is singular, then this undefined. So, we would generally say the condition number is infinite. On the other hand, we can quickly verify based on norm properties that\n",
    "$$\n",
    "\\kappa(A) := \\|A\\|\\|A^{-1}\\| \\geq \\|A A^{-1}\\| = 1\n",
    "$$\n",
    "So, matrices that are close to $1$ are extremely nice to have. As an example, the identity matrix has a condition number of exactly $1$, as it is its own inverse. Another example of a matrix with condition number $1$ is a permutation elementary matrix $P_{ij}$, discussed in the previous lecture. This intuitively makes sense: swapping rows has no impact on algebraic operations. \n",
    "\n",
    "In fact, there is one condition number for each choice of norm, so we will usually work with\n",
    "$$\n",
    "\\kappa_\\infty(A) := \\|A\\|_\\infty \\|A^{-1}\\|_\\infty\n",
    "$$\n",
    "However, there may be times we also work with the 2-norm or the Frobenius norm.\n",
    "\n",
    "In Python, a good approximation of the condition number $\\kappa_\\infty(A)$ is given by `numpy.linalg.cond(A, numpy.inf)`.\n",
    "<br>\n",
    "As with `numpy.linalg.norm`, the simple form `numpy.linalg.cond(A)` defaults to using the 2-norm, based on the Euclidian length $\\| \\cdot \\|_2$ for vectors.\n",
    "\n",
    "This is not done exactly, since computing the inverse is a lot of work for large matrices and good estimates can be found far more quickly.\n",
    "The basic idea is start with the formula\n",
    "$$\n",
    "\\| A^{-1} \\| = \\max_{\\|x\\|=1} \\| A ^{-1} x \\|\n",
    "$$\n",
    "and instead compute the maximum over some finite selection of values for $x$: call them $x^{(k)}$.\n",
    "Then to evaluate $y^{(k)} =  A ^{-1} x^{(k)}$, express this through the equation $A y^{(k)} = x^{(k)}$.\n",
    "Once we have an LU factorization for $A$ (which one probably would have when exploring errors in a numerical solution of $Ax = b$) each of these systems can be solved relatively fast:\n",
    "Then\n",
    "$$\n",
    "\\| A^{-1} \\| \\approx \\max_k \\| y^{(k)} \\|.\n",
    "$$\n",
    "\n",
    "Condition numbers, giving upper limit on the ratio of forward error to backward error,\n",
    "measure the amplification of errors, and have counterparts in other contexts.\n",
    "For example, with an approximation $r_a$ of a root $r$ of the equation $f(x) = 0$, the ratio of forward error to backward error is bounded by\n",
    "$\\displaystyle \\max 1/| f'(x) | = \\frac{1}{\\min | f'(x) |}$, where the maximum only need be taken over an interval known to contain both the root and the approximation.\n",
    "This condition number becomes \"infinite\" for a multiple root, $f'(r) = 0$.\n",
    "\n",
    "For a unique solution, careful calculation of an approximate solution $x_a$ of $Ax = b$ can often get a *residual* that is at the level of machine rounding error, so that roughly the relative backward error is of size comparable to the machine unit, $u$.\n",
    "The condition number then guarantees that the (forward) relative error is no greater than about $u \\, \\kappa(A)$.\n",
    "\n",
    "In terms of significant bits, with $p$ bit machine arithmetic, one can hope to get $p - \\log_2(\\kappa(A))$ significant bits in the result, but can not rely on more, so one loses $\\log_2(\\kappa(A))$ significant bits.\n",
    "\n",
    "A *well-conditioned problem* is one that is not too highly sensitive to errors in rounding or input data; for an eqution $Ax = b$, this corresponds to the condition number of $A$ not being to large; the matrix $A$ is then sometimes also called well-conditioned.\n",
    "This is of course vague, but might typically mean that $p - \\log_2(\\kappa(A))$ is a sufficient number of significant bits for a particular purpose.\n",
    "\n",
    "A problem that is not deemed well-conditioned is called *ill-conditioned*, so that a matrix of uncomfortably large condition number is also sometimes called ill-conditioned.\n",
    "An ill-conditioned problem might still be well-posed, but just requiring careful and precise solution methods.\n",
    "\n",
    "It's also important to note that even though a matrix is well-conditioned, it is important to consider the implementation of the algorithm for the problem at-hand. For example, returning to Gaussian elimination, consider the system of equations\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "\\epsilon &1 \\\\\n",
    "1 &1 \\\\\n",
    "\\end{bmatrix} \\begin{bmatrix}\n",
    "x_1 \\\\\n",
    "x_2 \\end{bmatrix} = \\begin{bmatrix}\n",
    "1 \\\\ \n",
    "0 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "with $\\epsilon \\ll 1$. We can compute the condition number using the max norm fairly easily. We have $\\kappa_\\infty = 4/(1-\\epsilon)$, which for $\\epsilon$ small, is close to 1.\n",
    "\n",
    "Further, we know the solution is exactly $x_1 = -x_2 = -1/(1-\\epsilon) \\approx -1$. However, in Gaussian elimination, we would multiply the first equation by $1/\\epsilon$ and subtract, giving $(1-1/\\epsilon)x_2 = -1/\\epsilon$. This may be rounded to $1$ if $\\epsilon$ is sufficiently small, which when plugged back in for $x_1$ would give $\\epsilon x_1 = 1-1 = 0$, which is incorrect. But, this can be corrected with pivoting. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overdetermined Linear System\n",
    "\n",
    "Given a $n \\times m$ matrix $A$, $n \\geq m$, and an $n$-vector $b$, we seek the $m$-vector $x$ that is the best approximate solution of $Ax = b$ in the sense that the residual $Ax - b$ is as small as possible. However, we should say under what norm we wish the solution to be small: in general, this is done by minimizing the residual of the 2-norm. So, we wish to compute $\\tilde{x}$ such that\n",
    "$$\n",
    "f(x) := \\|Ax - b\\|_2\n",
    "$$\n",
    "is a minimum. Note here that this ia the vector 2-norm. As it turns out, we can apply classic techniques from multivariable calculus to find the solution. First, find the critical points of $f$ by setting $\\nabla f(x) = 0$. Then, after solving, we can show with careful manipulation that the *Hessian* matrix $\\nabla^2 f$ has a strictly positive determinant, which justifies that it is a minimum. The details are left as an exercise. \n",
    "\n",
    "However, assuming those calculations were done, it turns out that the solution is\n",
    "$$(A^* A) \\tilde{x} = A^* b,$$\n",
    "which is now an $m \\times m$ system of equations with a unique solution — and it is the \"right\" one! Moreover, it can be shown that $A^* A$ is invertible as long as $A$ has independent columns.\n",
    "\n",
    "To outline the justification:\n",
    "\n",
    "1. Note that $A^* A$ is $m \\times m$. So, we have $A^* A$ is square. \n",
    "2. $A$ and $A^* A$ share the same nullspace, as\n",
    "$$\n",
    "A^* Ax = 0 \\iff x^* A^* A x = 0 \\iff (Ax)^* Ax = 0 \\iff \\|Ax\\|_2^2 = 0 \\iff Ax = 0\n",
    "$$\n",
    "\n",
    "Since $A^*A$ is square and nonsingular, it is invertible. Thus, we define the unique solution that minimizes the least-squares residual using the *Moore-Penrose* or *pseudo-inverse* $A^+ := (A^*A)^{-1}A^*$:\n",
    "$$\n",
    "\\tilde{x} = A^+ b \n",
    "$$\n",
    "\n",
    "In fact, this is commonly used to compute the condition number for non-square matrices in software packages, i.e. we can extend our definition of a condition number to be the following:\n",
    "\n",
    "$$\n",
    "\\kappa(A) := \\|A\\|\\|A^+\\| \n",
    "$$\n",
    "\n",
    "Notice that if $A$ is invertible, then in-fact $A^+$ reduces to $A^{-1}$. So, this is a natural extension of the condition number definition. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "### Exercise 1\n",
    "\n",
    "Consider the problem of solving $Ax=b$ in the least-squares sense where \n",
    "\n",
    "$$\n",
    "A = \\begin{bmatrix}\n",
    "1 &1 \\\\\n",
    "1 &1.0001 \\\\\n",
    "1 &1.0001 \n",
    "\\end{bmatrix}, \\; \\; \\; \\text{ and } \\; \\; \\; \n",
    "b = \\begin{bmatrix}\n",
    "2 \\\\\n",
    ".0001 \\\\\n",
    "4.0001 \n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "a) Calculate $A^+$ for the example above. Give exact answers. \n",
    "<br>\n",
    "b) Find the exact solution to $Ax=b$ by minimizing the residual under least squares \n",
    "<br>\n",
    "c) Calculate $\\kappa_\\infty(A)$ and $||A||_F$. Give exact answers. \n",
    "\n",
    "\n",
    "### Exercise 2\n",
    "\n",
    "Let $u$ and $v$ be two vectors and let $E$ be the *outer product*, i.e.\n",
    "\n",
    "$$\n",
    "E = u v^T = \\begin{bmatrix}\n",
    "u_1 v_1 &u_2 v_1 &... &u_n v_1 \\\\\n",
    "u_1 v_2 &u_2 v_2 &... &u_n v_2 \\\\\n",
    "\\vdots & &\\ddots &\\vdots \\\\\n",
    "u_1 v_n &u_2 v_n &... &u_n v_n\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Is it true that $\\|E\\|_F = \\|u\\|_F \\|v\\|_F$? Prove it or give a counterexample. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
